{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import time\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "import random\n",
    "import glob as gb\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "# import resnet50_128_redesign as model\n",
    "import resnet50_128B8 as model\n",
    "from PIL import Image\n",
    "from tqdm import tqdm,trange\n",
    "from torchsummary import summary\n",
    "from torch.optim import lr_scheduler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils import clip_grad_norm_, clip_grad_value_\n",
    "from torchvision import transforms, utils\n",
    "from sklearn.metrics import accuracy_score\n",
    "from DataProcessing import process_dataloder\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders, class_names, dataset_sizes, expA = process_dataloder()\n",
    "print(\"After augmentation\")\n",
    "print(dataset_sizes)\n",
    "print(class_names)\n",
    "print(expA) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cyclical_lr(stepsize, min_lr, max_lr):\n",
    "\n",
    "    # Scaler: we can adapt this if we do not want the triangular CLR\n",
    "    scaler = lambda x: 1.\n",
    "\n",
    "    # Lambda function to calculate the LR\n",
    "    lr_lambda = lambda it: min_lr + (max_lr - min_lr) * relative(it, stepsize)\n",
    "\n",
    "    # Additional function to see where on the cycle we are\n",
    "    def relative(it, stepsize):\n",
    "        cycle = math.floor(1 + it / (2 * stepsize))\n",
    "        x = abs(it / stepsize - 2 * cycle + 1)\n",
    "        return max(0, (1 - x)) * scaler(cycle)\n",
    "\n",
    "    return lr_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(phase, model, criterion, optimizer):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    nb_classes = 4\n",
    "    confusion_matrix = torch.zeros(nb_classes, nb_classes)\n",
    "\n",
    "    # Iterate over data.        \n",
    "    for inputs, labels in dataloaders[phase]:            \n",
    "        inputs = inputs.to(device)            \n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward\n",
    "        # track history if only in train\n",
    "        with torch.set_grad_enabled(phase == 'train'):\n",
    "            \n",
    "            outputs = model(inputs)            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)    \n",
    "            \n",
    "            # backward + optimize only if in training phase\n",
    "            if phase == 'train':\n",
    "                loss.backward()\n",
    "                clip_grad_norm_(model.parameters(), 5)\n",
    "                optimizer.step()  \n",
    "            if phase == 'test':              \n",
    "                with torch.no_grad():\n",
    "                    for t, p in zip(labels.view(-1), preds.view(-1)):\n",
    "                        confusion_matrix[t.long(), p.long()] += 1\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data) \n",
    "        \n",
    "    if phase == 'test':\n",
    "        print(confusion_matrix)\n",
    "\n",
    "    return running_loss, running_corrects     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_mode(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "    logs = []\n",
    "    Acc = {'train':[],'test':[]}\n",
    "    Los = {'train':[],'test':[]}\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):        \n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'test']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "            running_loss, running_corrects = training(phase, model, criterion, optimizer)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                lr_sched_test = scheduler.get_lr()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "            phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            # deep copy the model\n",
    "            if phase == 'test' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())      \n",
    "            Acc[phase].append(epoch_acc)\n",
    "            Los[phase].append(epoch_loss)\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best test Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, Acc, Los"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download the pre-trained model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# modelRes50 = model.resnet50_128(weights_path='./model/resnet50_128.pth')\n",
    "# modelRes50.add_module(\"feat_extract1\",nn.Conv2d(128, 64, kernel_size=[1, 1], stride=(1, 1), bias=False))\n",
    "# modelRes50.add_module(\"feat_extract2\",nn.Conv2d(64, 4, kernel_size=[1, 1], stride=(1, 1), bias=False))\n",
    "# modelRes50.add_layers([modelRes50.feat_extract1, modelRes50.feat_extract2])\n",
    "# modelRes50 = modelRes50.to(device)\n",
    "# features_layers = 91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Freeze layers before classifier\n",
    "def freezing(model, features_layers):\n",
    "    lay_mark = 0;\n",
    "    para_list = []\n",
    "    for param in modelRes50.parameters():\n",
    "        if lay_mark > features_layers:\n",
    "            para_list.append(param)\n",
    "        if lay_mark <= features_layers:\n",
    "            param.requires_grad = False\n",
    "        lay_mark += 1\n",
    "    return model, para_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download the pre-trained model\n",
    "features_layers = 91\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "modelRes50 = model_scr.resnet50_128(weights_path='./model/resnet50_128.pth')\n",
    "modelRes50.add_module(\"Add_pool\",nn.AvgPool2d(kernel_size=[14, 14], stride=[1, 1], padding=0))\n",
    "modelRes50.add_module(\"feat_extract0\",nn.Conv2d(1024, 4, kernel_size=[1, 1], stride=(1, 1), bias=False))\n",
    "modelRes50.add_layers([modelRes50.Add_pool, modelRes50.feat_extract0])\n",
    "modelRes50, para_list = freezing(modelRes50, features_layers)\n",
    "modelRes50 = modelRes50.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final layer are being optimized \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(para_list, lr=0.00035, momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelRes50, Acc, Los = epoch_mode(modelRes50, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
